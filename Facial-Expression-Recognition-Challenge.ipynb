{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, Model, model_from_json\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Activation, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fer2013.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training       28709\n",
       "PublicTest      3589\n",
       "PrivateTest     3589\n",
       "Name: Usage, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Usage\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion    0\n",
       "pixels     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df[[\"emotion\", \"pixels\"]][df[\"Usage\"] == \"Training\"]\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28709, 2304), (28709,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['pixels'] = train['pixels'].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "x_train = np.vstack(train['pixels'].values)\n",
    "y_train = np.array(train[\"emotion\"])\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_test_df = df[[\"emotion\", \"pixels\"]][df[\"Usage\"]==\"PublicTest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_test_df[\"pixels\"] = public_test_df[\"pixels\"].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "x_test = np.vstack(public_test_df[\"pixels\"].values)\n",
    "y_test = np.array(public_test_df[\"emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28709, 48, 48, 1), (3589, 48, 48, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(-1, 48, 48, 1)\n",
    "x_test = x_test.reshape(-1, 48, 48, 1)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28709, 7), (3589, 7))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 44, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_1 (Ba (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 44, 44, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_2 (Ba (None, 20, 20, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_3 (Ba (None, 18, 18, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_4 (Ba (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_5 (Ba (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 339,111\n",
      "Trainable params: 338,407\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, 3, data_format=\"channels_last\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape=(48, 48, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv2D(64, 3))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Conv2D(32, 3))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(32, 3))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(32, 3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Dropout(0.6))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28709 samples, validate on 3589 samples\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.63628, saving model to face_model.h5\n",
      "28709/28709 - 767s - loss: 2.0127 - accuracy: 0.2511 - val_loss: 1.6363 - val_accuracy: 0.3700\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.63628 to 1.50763, saving model to face_model.h5\n",
      "28709/28709 - 622s - loss: 1.6539 - accuracy: 0.3529 - val_loss: 1.5076 - val_accuracy: 0.4199\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.50763 to 1.49847, saving model to face_model.h5\n",
      "28709/28709 - 622s - loss: 1.5344 - accuracy: 0.4006 - val_loss: 1.4985 - val_accuracy: 0.4310\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.49847 to 1.40611, saving model to face_model.h5\n",
      "28709/28709 - 628s - loss: 1.4616 - accuracy: 0.4374 - val_loss: 1.4061 - val_accuracy: 0.4533\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.40611 to 1.33396, saving model to face_model.h5\n",
      "28709/28709 - 622s - loss: 1.4099 - accuracy: 0.4575 - val_loss: 1.3340 - val_accuracy: 0.4812\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.33396 to 1.28478, saving model to face_model.h5\n",
      "28709/28709 - 622s - loss: 1.3695 - accuracy: 0.4703 - val_loss: 1.2848 - val_accuracy: 0.5049\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.28478 to 1.25508, saving model to face_model.h5\n",
      "28709/28709 - 624s - loss: 1.3358 - accuracy: 0.4873 - val_loss: 1.2551 - val_accuracy: 0.5146\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.25508 to 1.25161, saving model to face_model.h5\n",
      "28709/28709 - 625s - loss: 1.3076 - accuracy: 0.4980 - val_loss: 1.2516 - val_accuracy: 0.5113\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.25161\n",
      "28709/28709 - 624s - loss: 1.2825 - accuracy: 0.5098 - val_loss: 1.2781 - val_accuracy: 0.5143\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.25161 to 1.19398, saving model to face_model.h5\n",
      "28709/28709 - 626s - loss: 1.2650 - accuracy: 0.5144 - val_loss: 1.1940 - val_accuracy: 0.5408\n"
     ]
    }
   ],
   "source": [
    "# save best weights\n",
    "checkpointer = ModelCheckpoint(filepath='face_model.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "# num epochs\n",
    "epochs = 10\n",
    "\n",
    "# run model\n",
    "hist = model.fit(x_train, y_train, epochs=epochs,\n",
    "                 shuffle=True,\n",
    "                 batch_size=100, validation_data=(x_test, y_test),\n",
    "                 callbacks=[checkpointer], verbose=2)\n",
    "\n",
    "# save model to json\n",
    "model_json = model.to_json()\n",
    "with open(\"face_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from model import FacialExpressionModel\n",
    "import numpy as np\n",
    "\n",
    "rgb = cv2.VideoCapture(0)\n",
    "facec = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "def __get_data__():\n",
    "    \"\"\"\n",
    "    __get_data__: Gets data from the VideoCapture object and classifies them\n",
    "    to a face or no face. \n",
    "    \n",
    "    returns: tuple (faces in image, frame read, grayscale frame)\n",
    "    \"\"\"\n",
    "    _, fr = rgb.read()\n",
    "    gray = cv2.cvtColor(fr, cv2.COLOR_BGR2GRAY)\n",
    "    faces = facec.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    return faces, fr, gray\n",
    "\n",
    "def start_app(cnn):\n",
    "    skip_frame = 10\n",
    "    data = []\n",
    "    flag = False\n",
    "    ix = 0\n",
    "    while True:\n",
    "        ix += 1\n",
    "        \n",
    "        faces, fr, gray_fr = __get_data__()\n",
    "        for (x, y, w, h) in faces:\n",
    "            fc = gray_fr[y:y+h, x:x+w]\n",
    "            \n",
    "            roi = cv2.resize(fc, (48, 48))\n",
    "            pred = cnn.predict_emotion(roi[np.newaxis, :, :, np.newaxis])\n",
    "\n",
    "            cv2.putText(fr, pred, (x, y), font, 1, (255, 255, 0), 2)\n",
    "            cv2.rectangle(fr,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "        cv2.imshow('Filter', fr)\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from disk\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 44, 44, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_1 (Ba (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 44, 44, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 22, 22, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_2 (Ba (None, 20, 20, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 20, 20, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_3 (Ba (None, 18, 18, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_4 (Ba (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_5 (Ba (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 903       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 339,111\n",
      "Trainable params: 338,407\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = FacialExpressionModel(\"face_model.json\", \"face_model.h5\")\n",
    "start_app(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
